{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nanth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#for text pre-processing\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "#for model-building\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "# bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#for word embedding\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from xgboost import XGBClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(r'..\\data\\BankFAQs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "insurance        469\n",
      "cards            403\n",
      "loans            375\n",
      "accounts         306\n",
      "investments      140\n",
      "security          57\n",
      "fundstransfer     14\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x=df_train['Class'].value_counts()\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>Class</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do I need to enter ‘#’ after keying in my Card...</td>\n",
       "      <td>Please listen to the recorded message and foll...</td>\n",
       "      <td>security</td>\n",
       "      <td>need enter key card number card expiry date cv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What details are required when I want to perfo...</td>\n",
       "      <td>To perform a secure IVR transaction, you will ...</td>\n",
       "      <td>security</td>\n",
       "      <td>detail require want perform secure ivr transac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How should I get the IVR Password  if I hold a...</td>\n",
       "      <td>An IVR password can be requested only from the...</td>\n",
       "      <td>security</td>\n",
       "      <td>get ivr password hold add card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do I register my Mobile number for IVR Pas...</td>\n",
       "      <td>Please call our Customer Service Centre and en...</td>\n",
       "      <td>security</td>\n",
       "      <td>register mobile number ivr password</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I obtain an IVR Password</td>\n",
       "      <td>By Sending SMS request: Send an SMS 'PWD&lt;space...</td>\n",
       "      <td>security</td>\n",
       "      <td>obtain ivr password</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  Do I need to enter ‘#’ after keying in my Card...   \n",
       "1  What details are required when I want to perfo...   \n",
       "2  How should I get the IVR Password  if I hold a...   \n",
       "3  How do I register my Mobile number for IVR Pas...   \n",
       "4                  How can I obtain an IVR Password    \n",
       "\n",
       "                                              Answer     Class  \\\n",
       "0  Please listen to the recorded message and foll...  security   \n",
       "1  To perform a secure IVR transaction, you will ...  security   \n",
       "2  An IVR password can be requested only from the...  security   \n",
       "3  Please call our Customer Service Centre and en...  security   \n",
       "4  By Sending SMS request: Send an SMS 'PWD<space...  security   \n",
       "\n",
       "                                          clean_text  \n",
       "0  need enter key card number card expiry date cv...  \n",
       "1  detail require want perform secure ivr transac...  \n",
       "2                     get ivr password hold add card  \n",
       "3                register mobile number ivr password  \n",
       "4                                obtain ivr password  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "df_train['clean_text'] = df_train['Question'].apply(lambda x: finalpreprocess(x))\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class_encoded\n",
       "3    469\n",
       "1    403\n",
       "5    375\n",
       "0    306\n",
       "4    140\n",
       "6     57\n",
       "2     14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_train['Class_encoded'] =  le.fit_transform(df_train['Class'])\n",
    "df_train['Class_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"Class_encoded\"],test_size=0.2,shuffle=True)\n",
    "#Word2Vec\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open(r\"..\\model_artifacts\\tfidf1.pkl\", \"wb\"))\n",
    "\n",
    "#building Word2Vec model\n",
    "# class MeanEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         # if a text is empty we should return a vector of zeros\n",
    "#         # with the same dimensionality as all the other vectors\n",
    "#         self.dim = len(next(iter(word2vec.values())))\n",
    "# def fit(self, X, y):\n",
    "#         return self\n",
    "# def transform(self, X):\n",
    "#         return np.array([\n",
    "#             np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "#                     or [np.zeros(self.dim)], axis=0)\n",
    "#             for words in X\n",
    "#         ])\n",
    "# df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']]\n",
    "# model = Word2Vec(df_train['clean_text_tok'],min_count=1)     \n",
    "#modelw = MeanEmbeddingVectorizer(model)\n",
    "# converting text to numerical data using Word2Vec\n",
    "# X_train_vectors_w2v = model.fit_transform(X_train_tok)\n",
    "# X_val_vectors_w2v = model.transform(X_test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        57\n",
      "           1       0.98      0.95      0.96        83\n",
      "           2       1.00      0.25      0.40         4\n",
      "           3       0.94      0.99      0.96        97\n",
      "           4       0.91      0.80      0.85        25\n",
      "           5       0.95      0.97      0.96        75\n",
      "           6       0.77      0.83      0.80        12\n",
      "\n",
      "    accuracy                           0.94       353\n",
      "   macro avg       0.92      0.82      0.84       353\n",
      "weighted avg       0.94      0.94      0.94       353\n",
      "\n",
      "Confusion Matrix: [[52  0  0  1  2  1  1]\n",
      " [ 0 79  0  1  0  1  2]\n",
      " [ 2  0  1  0  0  1  0]\n",
      " [ 0  0  0 96  0  1  0]\n",
      " [ 2  1  0  2 20  0  0]\n",
      " [ 1  0  0  1  0 73  0]\n",
      " [ 0  1  0  1  0  0 10]]\n",
      "f1_score : 0.9350351947918704\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print('f1_score :', f1_score(y_test,y_predict,average='weighted'))\n",
    "pickle.dump(lr_tfidf,open('..\\model_artifacts\\logistic_reg.pkl','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.89      0.90        57\n",
      "           1       0.95      0.92      0.93        83\n",
      "           2       1.00      0.25      0.40         4\n",
      "           3       0.82      0.95      0.88        97\n",
      "           4       0.78      0.56      0.65        25\n",
      "           5       0.95      0.93      0.94        75\n",
      "           6       0.75      0.75      0.75        12\n",
      "\n",
      "    accuracy                           0.89       353\n",
      "   macro avg       0.88      0.75      0.78       353\n",
      "weighted avg       0.89      0.89      0.88       353\n",
      "\n",
      "Confusion Matrix: [[51  1  0  3  1  0  1]\n",
      " [ 0 76  0  4  1  0  2]\n",
      " [ 2  0  1  0  0  1  0]\n",
      " [ 1  0  0 92  1  3  0]\n",
      " [ 2  2  0  7 14  0  0]\n",
      " [ 0  0  0  4  1 70  0]\n",
      " [ 0  1  0  2  0  0  9]]\n",
      "f1_score : 0.8827084930638133\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "\n",
    "lr_tfidf=XGBClassifier()\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "print('f1_score :', f1_score(y_test,y_predict,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label = df_train[['Class_encoded','Class',]].drop_duplicates().reset_index(drop=True)\n",
    "class_label_dict = {}\n",
    "for i in range(class_label.shape[0]):\n",
    "    class_label_dict[class_label.iat[i,0]] = class_label.iat[i,1]\n",
    "\n",
    "with open('..\\model_artifacts\\class_label_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(class_label_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "security\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"../\")\n",
    "\n",
    "from utils.preprocess import finalpreprocess\n",
    "import pickle\n",
    "\n",
    "with open('..\\model_artifacts\\class_label_dict.pkl', 'rb') as f:\n",
    "    class_label_dict = pickle.load(f)\n",
    "\n",
    "question = 'What details are required when I want to perform a secure IVR transaction'\n",
    "\n",
    "question_pp = finalpreprocess(question)\n",
    "tfidf_vectorizer = pickle.load(open(r'..\\model_artifacts\\tfidf1.pkl','rb'))\n",
    "question_vec = tfidf_vectorizer.transform([question_pp])\n",
    "lr_tfidf = pickle.load(open('..\\model_artifacts\\logistic_reg.pkl','rb')) \n",
    "pred = lr_tfidf.predict(question_vec)\n",
    "print(class_label_dict[pred[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
